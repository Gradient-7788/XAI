{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b97544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bbf6828",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "112a4ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      "=== Random Search for SimpleCNN ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Random Search for SimpleCNN:   0%|          | 0/5 [00:29<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 514\u001b[0m\n\u001b[0;32m    512\u001b[0m \u001b[38;5;66;03m# Perform random search for SimpleCNN\u001b[39;00m\n\u001b[0;32m    513\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Random Search for SimpleCNN ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 514\u001b[0m cnn_results, cnn_best_params \u001b[38;5;241m=\u001b[39m random_search(SimpleCNN, X_train, y_train, param_distributions, n_iter\u001b[38;5;241m=\u001b[39mn_trials, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    515\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mBest parameters for SimpleCNN: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcnn_best_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    516\u001b[0m plot_hyperparameter_results(cnn_results, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSimpleCNN\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[3], line 302\u001b[0m, in \u001b[0;36mrandom_search\u001b[1;34m(model_class, X, y, param_distributions, n_iter, device)\u001b[0m\n\u001b[0;32m    299\u001b[0m             hyperparams[param_name] \u001b[38;5;241m=\u001b[39m random\u001b[38;5;241m.\u001b[39muniform(min_val, max_val)\n\u001b[0;32m    301\u001b[0m \u001b[38;5;66;03m# Perform cross-validation\u001b[39;00m\n\u001b[1;32m--> 302\u001b[0m cv_score \u001b[38;5;241m=\u001b[39m train_with_cv(model_class, X, y, hyperparams, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m    304\u001b[0m \u001b[38;5;66;03m# Store results\u001b[39;00m\n\u001b[0;32m    305\u001b[0m hyperparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcv_score\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m cv_score\n",
      "Cell \u001b[1;32mIn[3], line 255\u001b[0m, in \u001b[0;36mtrain_with_cv\u001b[1;34m(model_class, X, y, hyperparams, n_splits, device)\u001b[0m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m X_batch, y_batch \u001b[38;5;129;01min\u001b[39;00m val_loader:\n\u001b[0;32m    254\u001b[0m     X_batch \u001b[38;5;241m=\u001b[39m X_batch\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m--> 255\u001b[0m     preds \u001b[38;5;241m=\u001b[39m model(X_batch)\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    256\u001b[0m     all_preds\u001b[38;5;241m.\u001b[39mextend(preds)\n\u001b[0;32m    257\u001b[0m     all_labels\u001b[38;5;241m.\u001b[39mextend(y_batch\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[3], line 99\u001b[0m, in \u001b[0;36mSimpleCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     98\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x))))\n\u001b[1;32m---> 99\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x))))\n\u001b[0;32m    100\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(x)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(x)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:375\u001b[0m, in \u001b[0;36mConv1d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 375\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conv_forward(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\conv.py:370\u001b[0m, in \u001b[0;36mConv1d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    358\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    359\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[0;32m    360\u001b[0m         F\u001b[38;5;241m.\u001b[39mpad(\n\u001b[0;32m    361\u001b[0m             \u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    368\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups,\n\u001b[0;32m    369\u001b[0m     )\n\u001b[1;32m--> 370\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv1d(\n\u001b[0;32m    371\u001b[0m     \u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups\n\u001b[0;32m    372\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load FordA dataset\n",
    "def load_forda_data():\n",
    "    train = np.loadtxt(\"FordA_TRAIN.txt\")\n",
    "    test = np.loadtxt(\"FordA_TEST.txt\")\n",
    "    \n",
    "    y_train, X_train = train[:, 0], train[:, 1:]\n",
    "    y_test, X_test = test[:, 0], test[:, 1:]\n",
    "    \n",
    "    # Standardize features\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_forda_data()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors and create DataLoaders\n",
    "def create_dataloader(X, y, batch_size=32):\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32).unsqueeze(1)  # add channel dim\n",
    "    y_tensor = torch.tensor((y > 0).astype(np.int64))  # convert -1/1 to 0/1\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Simple CNN with unique ReLU modules\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_channels=1, num_classes=2, filters1=16, filters2=32, dropout_rate=0.0):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(num_channels, filters1, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(filters1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(filters1, filters2, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(filters2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(filters2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout1(self.relu1(self.bn1(self.conv1(x))))\n",
    "        x = self.dropout2(self.relu2(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        return self.fc(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# ResNet Block with separate ReLUs\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, dropout_rate=0.0):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels, out_channels, kernel_size, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(out_channels, out_channels, kernel_size, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(out_channels)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        self.shortcut = (\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=1)\n",
    "            if in_channels != out_channels else None\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x if self.shortcut is None else self.shortcut(x)\n",
    "        out = self.dropout1(self.relu1(self.bn1(self.conv1(x))))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += residual\n",
    "        return self.dropout2(self.relu2(out))\n",
    "\n",
    "class ResNet1D(nn.Module):\n",
    "    def __init__(self, num_channels=1, num_classes=2, filters1=16, filters2=32, dropout_rate=0.0):\n",
    "        super(ResNet1D, self).__init__()\n",
    "        self.layer1 = ResBlock(num_channels, filters1, dropout_rate=dropout_rate)\n",
    "        self.layer2 = ResBlock(filters1, filters2, dropout_rate=dropout_rate)\n",
    "        self.pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(filters2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.pool(x).squeeze(-1)\n",
    "        return self.fc(x)\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "# Cross-validation training function\n",
    "def train_with_cv(model_class, X, y, hyperparams, n_splits=3, device=\"cpu\"):\n",
    "    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    cv_scores = []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        X_train_fold, X_val_fold = X[train_idx], X[val_idx]\n",
    "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "        \n",
    "        train_loader = create_dataloader(X_train_fold, y_train_fold, batch_size=hyperparams['batch_size'])\n",
    "        val_loader = create_dataloader(X_val_fold, y_val_fold, batch_size=hyperparams['batch_size'])\n",
    "        \n",
    "        # Initialize model with proper hyperparameters\n",
    "        if model_class.__name__ == 'SimpleCNN' or model_class.__name__ == 'ResNet1D':\n",
    "            model = model_class(\n",
    "                filters1=hyperparams['filters1'], \n",
    "                filters2=hyperparams['filters2'],\n",
    "                dropout_rate=hyperparams['dropout_rate']\n",
    "            )\n",
    "        else:\n",
    "            model = model_class()\n",
    "        \n",
    "        model.to(device)\n",
    "        \n",
    "        # Initialize optimizer based on hyperparams\n",
    "        if hyperparams['optimizer'] == 'adam':\n",
    "            optimizer = optim.Adam(\n",
    "                model.parameters(), \n",
    "                lr=hyperparams['lr'], \n",
    "                weight_decay=hyperparams['weight_decay'],\n",
    "                betas=(hyperparams['beta1'], 0.999)\n",
    "            )\n",
    "        elif hyperparams['optimizer'] == 'sgd':\n",
    "            optimizer = optim.SGD(\n",
    "                model.parameters(), \n",
    "                lr=hyperparams['lr'], \n",
    "                momentum=hyperparams['momentum'], \n",
    "                weight_decay=hyperparams['weight_decay'],\n",
    "                nesterov=hyperparams['nesterov']\n",
    "            )\n",
    "        \n",
    "        # Scheduler\n",
    "        if hyperparams['use_scheduler']:\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer, \n",
    "                mode='max',\n",
    "                factor=0.5, \n",
    "                patience=2\n",
    "            )\n",
    "        \n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "        best_val_acc = 0\n",
    "        patience_counter = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(hyperparams['epochs']):\n",
    "            model.train()\n",
    "            for X_batch, y_batch in train_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                output = model(X_batch)\n",
    "                loss = criterion(output, y_batch)\n",
    "                loss.backward()\n",
    "                \n",
    "                # Apply gradient clipping if specified\n",
    "                if hyperparams['clip_grad_norm'] > 0:\n",
    "                    nn.utils.clip_grad_norm_(model.parameters(), hyperparams['clip_grad_norm'])\n",
    "                    \n",
    "                optimizer.step()\n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "            # Validation\n",
    "            model.eval()\n",
    "            all_preds, all_labels = [], []\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in val_loader:\n",
    "                    X_batch = X_batch.to(device)\n",
    "                    preds = model(X_batch).argmax(dim=1).cpu().numpy()\n",
    "                    all_preds.extend(preds)\n",
    "                    all_labels.extend(y_batch.numpy())\n",
    "            \n",
    "            val_acc = accuracy_score(all_labels, all_preds)\n",
    "            \n",
    "            # Update scheduler if used\n",
    "            if hyperparams['use_scheduler']:\n",
    "                scheduler.step(val_acc)\n",
    "            \n",
    "            # Early stopping\n",
    "            if val_acc > best_val_acc:\n",
    "                best_val_acc = val_acc\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "                \n",
    "            if hyperparams['early_stopping'] and patience_counter >= hyperparams['patience']:\n",
    "                break\n",
    "        \n",
    "        cv_scores.append(best_val_acc)\n",
    "    \n",
    "    return np.mean(cv_scores)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Random search for hyperparameter tuning\n",
    "def random_search(model_class, X, y, param_distributions, n_iter=20, device=\"cpu\"):\n",
    "    results = []\n",
    "    \n",
    "    for i in tqdm(range(n_iter), desc=f\"Random Search for {model_class.__name__}\"):\n",
    "        # Randomly sample hyperparameters\n",
    "        hyperparams = {}\n",
    "        for param_name, param_dist in param_distributions.items():\n",
    "            if isinstance(param_dist, list):\n",
    "                hyperparams[param_name] = random.choice(param_dist)\n",
    "            elif isinstance(param_dist, tuple) and len(param_dist) == 3:\n",
    "                # For continuous distributions, we have (min, max, log_scale)\n",
    "                min_val, max_val, log_scale = param_dist\n",
    "                if log_scale:\n",
    "                    hyperparams[param_name] = 10 ** (random.uniform(np.log10(min_val), np.log10(max_val)))\n",
    "                else:\n",
    "                    hyperparams[param_name] = random.uniform(min_val, max_val)\n",
    "        \n",
    "        # Perform cross-validation\n",
    "        cv_score = train_with_cv(model_class, X, y, hyperparams, device=device)\n",
    "        \n",
    "        # Store results\n",
    "        hyperparams['cv_score'] = cv_score\n",
    "        results.append(hyperparams)\n",
    "        \n",
    "        print(f\"Trial {i+1}/{n_iter}, Score: {cv_score:.4f}, Params: {hyperparams}\")\n",
    "    \n",
    "    # Convert to DataFrame for easier analysis\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Find best hyperparameters\n",
    "    best_idx = results_df['cv_score'].idxmax()\n",
    "    best_params = results_df.iloc[best_idx].to_dict()\n",
    "    \n",
    "    return results_df, best_params\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Final evaluation on test set\n",
    "def evaluate_model(model_class, best_params, X_train, y_train, X_test, y_test, device=\"cpu\"):\n",
    "    # Create data loaders\n",
    "    train_loader = create_dataloader(X_train, y_train, batch_size=int(best_params['batch_size']))\n",
    "    test_loader = create_dataloader(X_test, y_test, batch_size=int(best_params['batch_size']))\n",
    "    \n",
    "    # Initialize model with best parameters\n",
    "    if model_class.__name__ == 'SimpleCNN' or model_class.__name__ == 'ResNet1D':\n",
    "        model = model_class(\n",
    "            filters1=int(best_params['filters1']), \n",
    "            filters2=int(best_params['filters2']),\n",
    "            dropout_rate=best_params['dropout_rate']\n",
    "        )\n",
    "    else:\n",
    "        model = model_class()\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    # Initialize optimizer\n",
    "    if best_params['optimizer'] == 'adam':\n",
    "        optimizer = optim.Adam(\n",
    "            model.parameters(), \n",
    "            lr=best_params['lr'], \n",
    "            weight_decay=best_params['weight_decay'],\n",
    "            betas=(best_params['beta1'], 0.999)\n",
    "        )\n",
    "    elif best_params['optimizer'] == 'sgd':\n",
    "        optimizer = optim.SGD(\n",
    "            model.parameters(), \n",
    "            lr=best_params['lr'], \n",
    "            momentum=best_params['momentum'], \n",
    "            weight_decay=best_params['weight_decay'],\n",
    "            nesterov=best_params['nesterov']\n",
    "        )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    if best_params['use_scheduler']:\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='max', factor=0.5, patience=2\n",
    "        )\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Train the model with best parameters\n",
    "    best_test_acc = 0\n",
    "    for epoch in range(int(best_params['epochs'])):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(X_batch)\n",
    "            loss = criterion(output, y_batch)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Apply gradient clipping if specified\n",
    "            if best_params['clip_grad_norm'] > 0:\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), best_params['clip_grad_norm'])\n",
    "                \n",
    "            optimizer.step()\n",
    "        \n",
    "        # Eval on test set\n",
    "        model.eval()\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in test_loader:\n",
    "                X_batch = X_batch.to(device)\n",
    "                preds = model(X_batch).argmax(dim=1).cpu().numpy()\n",
    "                all_preds.extend(preds)\n",
    "                all_labels.extend(y_batch.numpy())\n",
    "        acc = accuracy_score(all_labels, all_preds)\n",
    "        \n",
    "        if best_params['use_scheduler']:\n",
    "            scheduler.step(acc)\n",
    "            \n",
    "        if acc > best_test_acc:\n",
    "            best_test_acc = acc\n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), f\"best_{model_class.__name__}_model.pt\")\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}/{int(best_params['epochs'])}, Test Accuracy: {acc:.4f}\")\n",
    "    \n",
    "    # Load best model for final evaluation\n",
    "    model.load_state_dict(torch.load(f\"best_{model_class.__name__}_model.pt\"))\n",
    "    \n",
    "    # Final evaluation\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            preds = model(X_batch).argmax(dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(y_batch.numpy())\n",
    "    \n",
    "    final_acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"\\nFinal Test Accuracy with best parameters: {final_acc:.4f}\")\n",
    "    return model, final_acc\n",
    "\n",
    "# Visualize results\n",
    "def plot_hyperparameter_results(results_df, model_name):\n",
    "    # Select the top parameters for visualization\n",
    "    top_params = ['lr', 'batch_size', 'filters1', 'filters2', 'dropout_rate', 'weight_decay']\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i, param in enumerate(top_params):\n",
    "        if param in results_df.columns:\n",
    "            plt.subplot(2, 3, i+1)\n",
    "            plt.scatter(results_df[param], results_df['cv_score'])\n",
    "            \n",
    "            # Add trendline for continuous parameters\n",
    "            if len(results_df[param].unique()) > 10:\n",
    "                try:\n",
    "                    z = np.polyfit(results_df[param], results_df['cv_score'], 1)\n",
    "                    p = np.poly1d(z)\n",
    "                    plt.plot(sorted(results_df[param]), p(sorted(results_df[param])), \"r--\")\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "                # For log-scale parameters\n",
    "                if param in ['lr', 'weight_decay'] and results_df[param].min() > 0:\n",
    "                    plt.xscale('log')\n",
    "            \n",
    "            plt.xlabel(param)\n",
    "            plt.ylabel('CV Accuracy')\n",
    "            plt.title(f'{param} vs CV Accuracy')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{model_name}_hyperparameter_results.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot performance distribution\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(results_df['cv_score'], bins=20)\n",
    "    plt.xlabel('CV Accuracy')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(f'{model_name} Performance Distribution')\n",
    "    plt.axvline(results_df['cv_score'].max(), color='r', linestyle='--', \n",
    "                label=f'Best Score: {results_df[\"cv_score\"].max():.4f}')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'{model_name}_performance_distribution.png')\n",
    "    plt.show()\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # Set random seed for reproducibility\n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    \n",
    "    # Device setup\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Define parameter distributions for random search\n",
    "    param_distributions = {\n",
    "        # Discrete parameters\n",
    "        'batch_size': [8, 16, 32, 64, 128],\n",
    "        'epochs': [10, 15, 20, 30],\n",
    "        'filters1': [16, 32, 64, 128],\n",
    "        'filters2': [32, 64, 128, 256],\n",
    "        'optimizer': ['adam', 'sgd'],\n",
    "        'use_scheduler': [True, False],\n",
    "        'early_stopping': [True, False],\n",
    "        'patience': [3, 5, 7],\n",
    "        'nesterov': [True, False],\n",
    "        \n",
    "        # Continuous parameters (min, max, log_scale)\n",
    "        'lr': (1e-5, 1e-1, True),\n",
    "        'weight_decay': (1e-6, 1e-3, True),\n",
    "        'dropout_rate': (0.0, 0.5, False),\n",
    "        'momentum': (0.8, 0.99, False),\n",
    "        'beta1': (0.8, 0.99, False),\n",
    "        'clip_grad_norm': (0.0, 5.0, False),\n",
    "    }\n",
    "    \n",
    "    # Number of random trials\n",
    "    n_trials = 5  # Adjust based on available computational resources\n",
    "    \n",
    "    # Perform random search for SimpleCNN\n",
    "    print(\"\\n=== Random Search for SimpleCNN ===\")\n",
    "    cnn_results, cnn_best_params = random_search(SimpleCNN, X_train, y_train, param_distributions, n_iter=n_trials, device=device)\n",
    "    print(f\"\\nBest parameters for SimpleCNN: {cnn_best_params}\")\n",
    "    plot_hyperparameter_results(cnn_results, \"SimpleCNN\")\n",
    "    \n",
    "    # Perform random search for ResNet1D\n",
    "    print(\"\\n=== Random Search for ResNet1D ===\")\n",
    "    resnet_results, resnet_best_params = random_search(ResNet1D, X_train, y_train, param_distributions, n_iter=n_trials, device=device)\n",
    "    print(f\"\\nBest parameters for ResNet1D: {resnet_best_params}\")\n",
    "    plot_hyperparameter_results(resnet_results, \"ResNet1D\")\n",
    "    \n",
    "    # Evaluate models with best parameters\n",
    "    print(\"\\n=== Evaluating SimpleCNN with best parameters ===\")\n",
    "    best_cnn_model, cnn_acc = evaluate_model(SimpleCNN, cnn_best_params, X_train, y_train, X_test, y_test, device=device)\n",
    "    \n",
    "    print(\"\\n=== Evaluating ResNet1D with best parameters ===\")\n",
    "    best_resnet_model, resnet_acc = evaluate_model(ResNet1D, resnet_best_params, X_train, y_train, X_test, y_test, device=device)\n",
    "    \n",
    "    # Compare models\n",
    "    print(\"\\n=== Model Comparison ===\")\n",
    "    print(f\"SimpleCNN Test Accuracy: {cnn_acc:.4f}\")\n",
    "    print(f\"ResNet1D Test Accuracy: {resnet_acc:.4f}\")\n",
    "    \n",
    "    # Save best hyperparameters\n",
    "    cnn_results.to_csv('simplecnn_random_search_results.csv', index=False)\n",
    "    resnet_results.to_csv('resnet1d_random_search_results.csv', index=False)\n",
    "    \n",
    "    # Print summary of best configurations\n",
    "    print(\"\\n=== Best Configurations ===\")\n",
    "    print(\"SimpleCNN:\")\n",
    "    for param, value in cnn_best_params.items():\n",
    "        print(f\"  {param}: {value}\")\n",
    "    \n",
    "    print(\"\\nResNet1D:\")\n",
    "    for param, value in resnet_best_params.items():\n",
    "        print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5858cb5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
